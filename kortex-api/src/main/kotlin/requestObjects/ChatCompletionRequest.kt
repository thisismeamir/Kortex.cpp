package requestObjects

import kotlinx.serialization.Serializable
import pseudo.request.objects.Audio
import pseudo.request.objects.ResponseFormat
import pseudo.request.objects.StreamOption
import pseudo.request.objects.message.Message
import pseudo.request.objects.tool.Tool
import pseudo.request.objects.tool.ToolChoice


@Serializable
/**
 * Creates a model response for the given conversation.
 * The following parameters are not working for the `TensorRT-LLM` engine,
 * `frequencyPenalty`, `presencePenalty`, `topP`.
 *
 * @param audio Parameters for audio output. Required when audio output is requested.
 * @see Audio
 * @param dynatemp_exponent Dynamic temperature exponent. This parameter is only supported by `llama-cpp` engine.
 * @param dynatemp_range Dynamic temperature range. This parameter is only supported by `llama-cpp` engine.
 * @param frequency_penalty Modifies the likelihood of the model repeating the same words or phrases within a single output.
 * @param ignore_eos Ignores the end-of-sequence token (true or false). This parameter only supported by `llama-cpp` engine.
 * @param logit_bias Modify the likelihood of specified tokens appearing in the completion.
 * Accepts a JSON object that maps tokens (specified by their token ID in the
 * tokenizer) to an associated bias value from -100 to 100. Mathematically,
 * the bias is added to the logits generated by the model prior to sampling.
 * The exact effect will vary per model, but values between -1 and 1 should
 * decrease or increase likelihood of selection; values like -100 or 100
 * should result in a ban or exclusive selection of the relevant token.
 * @param logprobs Whether to return log probabilities of the output tokens or not.
 * If true, returns the log probabilities of each output token returned in the content of message.
 * @param max_completion_tokens Sets the upper limit on the number of tokens the model can generate in a single output.
 * @param max_tokens Sets the upper limit on the number of tokens the model can generate in a single output. This value is now deprecated in favor of maxCompletionTokens.
 * @param messages Array of chat messages to be used for generating the chat completion.
 * @see Message
 * @param metadata Developer-defined tags and values used for filtering completions in the dashboard.
 * @param min_keep Minimum number of tokens to keep. This parameter only supported by llama-cpp engine.
 * @param min_p Minimum probability threshold for token sampling. This parameter only supported by llama-cpp engine.
 * @param mirostat Whether to use Mirostat sampling or not. This parameter only supported by llama-cpp engine.
 * @param mirostat_eta Learning rate for Mirostat sampling. This parameter only supported by llama-cpp engine.
 * @param mirostat_tau Target entropy for Mirostat sampling. This parameter only supported by llama-cpp engine.
 * @param modalities Specifies the modalities (types of input) supported by the model. Currently, cortex only support text modalities.
 * @param model The unique identifier of the model.
 * @param n How many chat completion choices to generate for each input message.
 * Note that you will be charged based on the number of generated tokens across all the choices.
 * @param n_probs Number of probabilities to return. This parameter only supported by llama-cpp engine.
 * @param parallel_tool_calls Whether to enable parallel function calling during tool use. Cortex support parallel tool calls by default.
 * @param penalize_nl Penalizes newline tokens (true or false). This parameter only supported by llama-cpp engine.
 * @param presence_penalty Modifies the likelihood of the model including new tokens that are the same as tokens in the input.
 * @param repeat_last_n Number of previous token to penalize for repeating. This parameter is only supported by llama-cpp engine.
 * @param repeat_penalty Penalty for repeating tokens. This parameter only supported by llama-cpp engine.
 * @param response_format Format of the response. The default value is TEXT.
 * @see ResponseFormat
 * @param seed Seed for random number generation. This parameter only supported by llama-cpp engine.
 * @param service_tier Specifies the latency tier to use for processing the request.
 * This parameter is relevant for customers subscribed to the scale tier service:
 * If set to 'auto', and the Project is Scale tier enabled, the system will utilize scale tier credits until they are exhausted.
 * If set to 'auto', and the Project is not Scale tier enabled, the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee.
 * If set to 'default', the request will be processed using the default service tier with a lower uptime SLA and no latency guarentee. When not set, the default behavior is 'auto'. When this parameter is set, the response body will include the service_tier utilized.
 * We are actively working on this feature to bring cortex as fully OpenAI compatible platform.
 * Planning and roadmap for this feature can be found here.
 * @param stop Defines specific tokens or phrases that signal the model to stop producing further output.
 * @param store Whether to store the output of this chat completion request for use in our model distillation or evals products.
 * @param stream Determines the format for output generation.
 * If set true, the output is generated continuously, allowing for real-time streaming of responses.
 * If set false, the output is delivered in a single JSON file.
 * @param stream_options Options for streaming response. Only set this when you set stream: true.
 * @see StreamOption
 * @param temperature Controls the randomness of the model by scaling the logits before sampling.
 * @param tfs_z The z-score used for Typical token sampling. This parameter only supported by llama-cpp engine.
 * @param tool_choice The tool to use for the completion.
 * @see ToolChoice
 * @param tools List of tools to use for the completion.
 * @param top_k The number of highest probability vocabulary tokens to keep for top-k sampling.
 * @param top_p The cumulative probability threshold for nucleus sampling. This parameter only supported by llama-cpp engine.
 * @param top_logprobs Number of log probabilities to return. This parameter only supported by llama-cpp engine.
 * @param typ_p The probability of sampling a typical token. This parameter only supported by llama-cpp engine.
 * @param user The unique identifier of the user.
 */
data class ChatCompletionRequest(
    val audio: Audio?,
    val dynatemp_exponent: Double?,
    val dynatemp_range: Double?,
    val frequency_penalty: Double?,
    val ignore_eos: Boolean?,
    val logit_bias: Map<String, Double>?,
    val logprobs: Boolean? = false,
    val max_completion_tokens: Double?,
    val max_tokens: Double? = max_completion_tokens,
    val messages: List<Message>,
    val metadata: Map<String, String>?,
    val min_keep: Int?,
    val min_p: Double?,
    val mirostat: Boolean?,
    val mirostat_eta: Double?,
    val mirostat_tau: Double?,
    val modalities: List<String>?,
    val model: String,
    val n: Int = 1,
    val n_probs: Int?,
    val parallel_tool_calls: Boolean?,
    val penalize_nl: Boolean?,
    val presence_penalty: Double? = 0.6,
    val repeat_last_n: Int?,
    val repeat_penalty: Double?,
    val response_format: String = ResponseFormat.TEXT.name.lowercase(),
    val seed: Int = 123,
    val service_tier: String?,
    val stop: List<String>?,
    val store: Boolean? = false,
    val stream: Boolean = false,
    val stream_options: StreamOption?,
    val temperature: Double? = 0.8,
    val tfs_z: Double?,
    val tool_choice: ToolChoice?,
    val tools: List<Tool>?,
    val top_k: Int?,
    val top_logprobs: Int?,
    val top_p: Double? = 0.95,
    val typ_p: Double?,
    val user: String?
)


